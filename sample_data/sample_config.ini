[Global]

; Size of the reserved memory in MiB to be used for the forward calculation.
forward_memory_mb=2048

; Size of the reserved memory in MiB to be used for the backward calculation.
; Basically this value could be specified as same as `forward_memory_mb`.
backward_memory_mb=2048

; Size of the reserved memory in MiB to be used for the network parameters.
parameter_memory_mb=2048

; Seed value for the internal randomizer of NMTKit.
random_seed=12345

; Seed value for the neural network backend.
; If this value is 0, NMTKit automatically chooses an actual seed randomly.
backend_random_seed=12345


[Corpus]

; Following parameters could be specified using both abstract/relative paths.
; If users used relative paths, the "current directory" is used as the root
; location.

; Location of the parallel corpus for the training.
train_source=submodules/small_parallel_enja/train.en
train_target=submodules/small_parallel_enja/train.ja

; Location of the parallel corpus for the parameter validation.
dev_source=submodules/small_parallel_enja/dev.en
dev_target=submodules/small_parallel_enja/dev.ja

; Location of the parallel corpus for the testing.
test_source=submodules/small_parallel_enja/test.en
test_target=submodules/small_parallel_enja/test.ja


[Model]

; Vocabulary type in each side. Available options:
; * word ........ UTF-8 whitespace-separated words.
; * character ... UTF-8 letters including whitespaces.
source_vocabulary_type=word
target_vocabulary_type=word

; Vocabulary size in each side.
source_vocabulary_size=5000
target_vocabulary_size=7000

; Number of units in each embedding layer.
source_embedding_size=512
target_embedding_size=512

; Number of units in each RNN hidden layer.
; These values are basically not equal to the actual hidden layer sizes, and
; they are specified by both these values and encoder/decoder implementations.
encoder_hidden_size=512
decoder_hidden_size=512

; Attention strategy. Available options:
; * mlp ........ Multilayer perceptron-based model.
;                (proposed in [Bahdanau+14])
; * bilinear ... Bilinear-based model.
;                (proposed as the "general" method in [Luong+15])
attention_type=mlp

; Number of units in attention hidden layer.
; Currently, this value is used only in the "mlp" method.
attention_hidden_size=512


[Train]

; Hyperparameters for the Adam optimizer.
adam_alpha=0.001
adam_beta1=0.9
adam_beta2=0.999
adam_eps=1e-8

; Maximum number of words in the source/target sentences.
max_length=32

; Maximum ratio of the lengths between source/target sentences.
max_length_ratio=3.0

; Maximum number of target words in the batch data.
; Each batch data would be constructed including as many as samples within this
; limitation.
num_words_in_batch=2048

; Maximum number of batch data to be trained.
max_iteration=10000

; Evaluation (validation/testing) processes would be performed in each following
; step.
evaluation_interval=100
